# -*- coding: utf-8 -*-
"""bike_casual_registered_minmaxscalerhodloutmini.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/gist/kelintang/4ba84da085c2fa433a4f59ee565fdd77/bike_casual_registered_minmaxscalerhodloutmini.ipynb
"""

#Libiraies
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt
from sklearn.model_selection import GridSearchCV
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor

#imprt dataset
bike = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes.csv')

import datetime
day=[]
month=[]
year=[]
for t in bike['dteday']:
    date = datetime.datetime.strptime(t, '%m/%d/%y')
    day.append(date.day)
    month.append(date.month)
    year.append(date.year)
bike['day'] = pd.DataFrame(day)
bike['month'] = pd.DataFrame(month)
bike['year'] = pd.DataFrame(year)
bike["season"]=bike["season"].map({ 1: 'Winter',2: 'Spring',3: 'Summer',4: 'Fall'})
bike["weathersit"]=bike["weathersit"].map({ 1: 'Clear',2: 'Mist',3: 'Light Snow',4: 'Heavy Rain'})

bike = pd.get_dummies(bike, columns=['season', 'weathersit'])

# fix feels like c error
bike.loc[bike['temp_c'] - bike['feels_like_c'] > 20, 'feels_like_c'] = bike['temp_c']

bike['feel_diff_c'] = bike['temp_c'] - bike['feels_like_c']

bike[bike['feel_diff_c'] < - 3000]

# fit scaler on training data
norm_temp = MinMaxScaler().fit(bike[['temp_c']])
# transform training data
bike['temp_c_scaled'] = norm_temp.transform(bike[['temp_c']])

norm_feels_like = MinMaxScaler().fit(bike[['feels_like_c']])
# transform training data
bike['feels_like_c_scaled'] = norm_feels_like.transform(bike[['feels_like_c']])

# Split the data
X = bike.drop(["casual", "registered", "dteday"], axis = 1)
y = bike[["casual", "registered"]]
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .2, random_state = 42)

# Separate features and labels
#train_features = train_dataset.drop(['casual', 'dteday', 'registered'], axis=1)
#train_labels = train_dataset[['casual', 'registered']]
#test_features = test_dataset.drop(['casual', 'dteday', 'registered'], axis=1)
#test_labels = test_dataset[['casual', 'registered']]

# Apply one-hot encoding to categorical variables
#encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')

# Fit the encoder on the training features
#encoder.fit(X_train)

# Transform both training and test features
#X_train_encoded = encoder.transform(X_train)
#X_test_encoded = encoder.transform(X_test)

# Convert to DataFrame
#X_train = pd.DataFrame(X_train_encoded, columns=encoder.get_feature_names_out(X.columns))
#X_test = pd.DataFrame(X_test_encoded, columns=encoder.get_feature_names_out(X.columns))

#X_train.info()

# Scale the data
scaler = MinMaxScaler()
X_train = scaler.fit_transform(X_train) # fit the scale to the training data
X_test = scaler.transform(X_test) # use the same scale on the testing data

# This shows (number of samples, number of features)
#test_features_tensor.shape

#print(train_features_encoded.shape)
#print(test_features_encoded.shape)

features = X_train.shape[1]

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Input
def build_model():
    # Define model layers.
    input_layer = Input(shape=(features,))
    first_dense = Dense(units='800', activation='leaky_relu')(input_layer)
    second_dense = Dense(units='400', activation='leaky_relu')(first_dense)
    #third_dense = Dense(units='800', activation='leaky_relu')(second_dense)
    #fourth_dense = Dense(units='8', activation='relu')(third_dense)
    # Y output will be fed from the first dense
    y_output = Dense(units='2', activation = 'linear')(second_dense)
    # Define the model with the input layer
    # and a list of output layers
    model = Model(inputs=input_layer,outputs=y_output)
    return model

early_stopp = EarlyStopping(monitor='val_loss', min_delta=0.005, patience=3, verbose=0, mode='auto')

model = build_model()

# Specify the optimizer, and compile the model with loss functions for both outputs
model.compile(loss='MSE', optimizer= 'Adam', metrics=['mean_squared_error'])

# Train the model
history = model.fit(X_train, y_train , epochs = 200, batch_size = 32, validation_data = (X_test, y_test), callbacks=[early_stopp])

# Evaluate the model on the training data
_, train_mse = model.evaluate(X_train, y_train, verbose = 1)

# Evaluate the model on the testing data
_, test_mse = model.evaluate(X_test, y_test, verbose = 1)

# Get predictions for the testing data
predictions = model.predict(X_test)

# Get the r^2
from sklearn.metrics import r2_score
y2 = r2_score(y_test, predictions)
print(y2)

# Plot loss during training (we can do this because we saved a "history" during training)
from matplotlib import pyplot
pyplot.subplot(211)
pyplot.title('Loss')
pyplot.plot(history.history['loss'], label='train')
pyplot.plot(history.history['val_loss'], label='test')
pyplot.legend()
pyplot.ylim(0,13000)

holdout = pd.read_csv('https://raw.githubusercontent.com/byui-cse/cse450-course/master/data/bikes_december.csv')

import datetime
day=[]
month=[]
year=[]
for t in holdout['dteday']:
    date = datetime.datetime.strptime(t, '%m/%d/%y')
    day.append(date.day)
    month.append(date.month)
    year.append(date.year)
holdout['day'] = pd.DataFrame(day)
holdout['month'] = pd.DataFrame(month)
holdout['year'] = pd.DataFrame(year)
holdout["season"]=holdout["season"].map({ 1: 'Winter',2: 'Spring',3: 'Summer',4: 'Fall'})
holdout["weathersit"]=holdout["weathersit"].map({ 1: 'Clear',2: 'Mist',3: 'Light Snow',4: 'Heavy Rain'})

holdout = pd.get_dummies(holdout, columns=['season', 'weathersit'])

# fix feels like c error
holdout.loc[holdout['temp_c'] - holdout['feels_like_c'] > 20, 'feels_like_c'] = holdout['temp_c']

holdout['feel_diff_c'] = holdout['temp_c'] - holdout['feels_like_c']

holdout[holdout['feel_diff_c'] < - 3000]

# fit scaler on training data
norm_temp = MinMaxScaler().fit(holdout[['temp_c']])
# transform training data
holdout['temp_c_scaled'] = norm_temp.transform(holdout[['temp_c']])

norm_feels_like = MinMaxScaler().fit(holdout[['feels_like_c']])
# transform training data
holdout['feels_like_c_scaled'] = norm_feels_like.transform(holdout[['feels_like_c']])

# Split the data
holdoutmini = holdout.drop(["dteday"], axis = 1)

holdout = holdout.reindex(columns = X.columns)
holdout = holdout.fillna(0)

# Scale the data
#holdoutmini_scaler = MinMaxScaler()
holdout = scaler.transform(holdout) # fit the scale to the training data

holdout_y = model.predict(holdout)
df_predictions = pd.DataFrame(holdout_y, columns=[['casual', 'registered']])
df_predictions.to_csv('predictions.csv', index=False)

from google.colab import files
files.download("predictions.csv")