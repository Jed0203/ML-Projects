# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/189zvJZY-W4i_scCo-0QNMgqjkbc4zeNK
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns
from plotly import graph_objs as go
import plotly.express as px
# Input data files are available in the read-only "../input/" directory
# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory

import os
for dirname, _, filenames in os.walk('/kaggle/input'):
    for filename in filenames:
        print(os.path.join(dirname, filename))

df = pd.read_csv("/content/spam.csv",encoding = "ISO-8859-1")
df.head()

df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis='columns',inplace=True)
df.shape,df.info()

msg_cnts = [df[df['v1']=='spam'].shape[0],df[df['v1']=='ham'].shape[0]]
x_label=['SPAM','HAM']
sns.barplot(x=x_label, y=msg_cnts)
plt.title("spam and ham count")
plt.xlabel("message type")
plt.ylabel("amount of messages")
plt.show()

labels = ["spam","ham"]
plt.pie(x=msg_cnts,explode = [0,0.1],labels=labels,autopct='%.0f%%')
plt.title("spam and ham in percentage")
plt.legend()
plt.show()

#Here from this graph it is easily identical it is imbalanced dataset, so to make it balanced we are adding new dataset into this notebook.

def unlabeler(lis):
    temp = []
    for dp in lis:
        if dp == 0:
            temp.append('ham')
        else:
            temp.append('spam')
    return temp

new1 = pd.read_csv("/content/spam_ham_dataset.csv")
new1_df = new1[new1['label']=='spam']
new2 = pd.read_csv("/content/spam_or_not_spam.csv")
new2_df = new2[new2['label']==1]

# first dataframe
new_df = new1_df.drop(["Unnamed: 0",'label_num'],axis='columns')
new_df.columns = df.columns

# second dataframe
new2_df['v1'] = unlabeler(new2_df['label'])
new2_df['v2'] = new2['email']
new2_df = new2_df.drop(['email','label'],axis='columns')
new2_df.head()

new_df.head()

balanced_df = pd.concat([df,new_df,new2_df],axis='rows')
balanced_df.head()

df = balanced_df.copy()
df.isnull().sum(),df.shape

df.dropna(inplace=True)

msg_cnts = [df[df['v1']=='spam'].shape[0],df[df['v1']=='ham'].shape[0]]
labels = ["spam","ham"]
plt.pie(x=msg_cnts,explode = [0,0.1],labels=labels,autopct='%.0f%%')
plt.title("spam and ham in percentage")
plt.legend()
plt.show()

import nltk
import re
from urllib.parse import urlparse
from nltk.stem import WordNetLemmatizer
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.preprocessing import LabelEncoder
import warnings
warnings.filterwarnings('ignore')

nltk.download('stopwords')

lemmatizer = WordNetLemmatizer()
stop_words = list(stopwords.words('english'))
print(stop_words)

def textPocess(sms):
    try:
        # brackets replacing by space
        sms = re.sub('[][)(]',' ',sms)

        # url removing
        sms = [word for word in sms.split() if not urlparse(word).scheme]
        sms = ' '.join(sms)

        # removing words starts from @
        sms = re.sub(r'\@\w+','',sms)

        # removing html tags
        sms = re.sub(re.compile("<.*?>"),'',sms)

        # getting only characters and numbers
        sms = re.sub('[^A-Za-z0-9]',' ',sms)

        # make all words into lowercase
        sms = sms.lower()

        # word tokennization
        tokens = word_tokenize(sms,language='english')

        # removing whitespaces
        sms = [word.strip() for word in tokens]

        # removing word and number combinations or numbers
#         sms = [word for word in sms if not re.search('\d.',word)]

        # stopwords removing
        sms = [word for word in sms if word not in stop_words]

        # lemmatization
        sms = [lemmatizer.lemmatize(word) for word in sms]
        sms = ' '.join(sms)

        return sms
    except Exception as e:
        print("sms",sms)
        print("Error",e)
        return 0

nltk.download('punkt')
nltk.download('wordnet')

df['processed'] = df['v2'].apply(lambda sms: textPocess(sms))

encoder = LabelEncoder()
df['encoded'] = encoder.fit_transform(df['v1'])
df.head()

from wordcloud import WordCloud
spam_df = df[df['v1']=='spam']
ham_df = df[df['v1']=='ham']

wc = WordCloud(background_color = "black", width = 1200, height = 600,
               contour_width = 0, contour_color = "#410F01", max_words = 1000,
               scale = 1, collocations = False, repeat = True, min_font_size = 1)

text_data = [data for data in df['processed']]
text_data = ' '.join(text_data)
wc.generate(text_data)
plt.figure(figsize=[10,7])
plt.title("Top words in all smss")
plt.imshow(wc)
plt.axis('off')
plt.show()

text_data = [data for data in spam_df['processed']]
text_data = ' '.join(text_data)
wc.generate(text_data)
plt.figure(figsize=[10,7])
plt.title("Spam smss top words")
plt.imshow(wc)
plt.axis('off')
plt.show()

text_data = [data for data in ham_df['processed']]
text_data = ' '.join(text_data)
wc.generate(text_data)
plt.figure(figsize=[10,7])
plt.title("ham smss top words")
plt.imshow(wc)
plt.axis('off')
plt.show()

from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer
from sklearn.model_selection import train_test_split,cross_val_score
MIN_DF = 10

bow_vec = CountVectorizer(min_df=MIN_DF)
bow_vec_class = bow_vec.fit_transform(df['processed'])
bow_vec_arr = bow_vec_class.toarray()
bow_df = pd.DataFrame(bow_vec_arr,columns=bow_vec.get_feature_names_out())
bow_df.head()

tf_vec = TfidfVectorizer(min_df=MIN_DF)
tf_vec_class = tf_vec.fit_transform(df['processed'])
tf_vec_arr = tf_vec_class.toarray()
tf_df = pd.DataFrame(tf_vec_arr,columns=tf_vec.get_feature_names_out())
tf_df.head()

tf_df.shape,bow_df.shape,df.shape

from sklearn.model_selection import train_test_split,cross_val_score
from sklearn.naive_bayes import BernoulliNB
from sklearn.linear_model import LogisticRegression

X_train_b,X_test_b,y_train_b,y_test_b = train_test_split(bow_df,df['encoded'],stratify=df['encoded'])
X_train_t,X_test_t,y_train_t,y_test_t = train_test_split(tf_df,df['encoded'],stratify=df['encoded'])
print("X_train_b.shape,y_test_b.shape",X_train_b.shape,y_test_b.shape)
print("X_train_t.shape,y_test_t.shape",X_train_t.shape,y_test_t.shape)

model_nb_ber = BernoulliNB()
model_nb_ber.fit(X_train_b,y_train_b)
model_nb_ber.score(X_test_b,y_test_b)

model_nb_ber = BernoulliNB()
model_nb_ber.fit(X_train_t,y_train_t)
model_nb_ber.score(X_test_t,y_test_t)

bag_cv_score = cross_val_score(BernoulliNB(),bow_df,df['encoded'])
tf_cv_score = cross_val_score(BernoulliNB(),tf_df,df['encoded'])
print("score for BOW",bag_cv_score,"average",sum(bag_cv_score)/len(bag_cv_score))
print("score for tf-idf",tf_cv_score,"average",sum(tf_cv_score)/len(tf_cv_score))

model_nb_ber = LogisticRegression()
model_nb_ber.fit(X_train_b,y_train_b)
model_nb_ber.score(X_test_b,y_test_b)

bag_cv_score = cross_val_score(LogisticRegression(),bow_df,df['encoded'])
tf_cv_score = cross_val_score(LogisticRegression(),tf_df,df['encoded'])
print("score for BOW",bag_cv_score,"average",sum(bag_cv_score)/len(bag_cv_score))
print("score for tf-idf",tf_cv_score,"average",sum(tf_cv_score)/len(tf_cv_score))

model_nb_ber = LogisticRegression()
model_nb_ber.fit(X_train_t,y_train_t)
model_nb_ber.score(X_test_t,y_test_t)

# final training model
model = LogisticRegression()
model.fit(tf_df,df['encoded'])

def textProcess(sms):
    try:
        # brackets replacing by space
        sms = re.sub('[][)(]',' ',sms)

        # url removing
        sms = [word for word in sms.split() if not urlparse(word).scheme]
        sms = ' '.join(sms)

        # removing words starts from @
        sms = re.sub(r'\@\w+','',sms)

        # removing html tags
        sms = re.sub(re.compile("<.*?>"),'',sms)

        # getting only characters and numbers
        sms = re.sub('[^A-Za-z0-9]',' ',sms)

        # make all words into lowercase
        sms = sms.lower()

        # word tokennization
        tokens = word_tokenize(sms,language='english')

        # removing whitespaces
        sms = [word.strip() for word in tokens]

#         # removing word and number combinations or numbers
#         sms = [word for word in sms if not re.search('\d.',word)]

        # stopwords removing
        sms = [word for word in sms if word not in stop_words]

        # lemmatization
        sms = [lemmatizer.lemmatize(word) for word in sms]
        sms = ' '.join(sms)

        return sms
    except Exception as e:
        print("sms",sms)
        print("Error",e)
        return 0

def manager(sms):
    sms = textProcess(sms)
    sms = tf_vec.transform([sms])
    result = model.predict(sms)
    return result

sms1 = """100% of daily 1.50 GB data quota exhausted as on 26-May-23 00:31.
Jio Number : Daily high speed data quota will be restored on 26-May-23 01:04.
To know where you have consumed your data quota, click """

sms2 = """Hurry! Recharge Jio no.XXXXXXXXXX on PhonePe & get rewards upto Rs.500
each on first 3 recharges for self & family. Recharge with Rs.239 plan .T&CA.
Click https://phon.pe/jion1"""

sms3 = """Hi XXXXXXXXX,
Prize pool - Rs. 5,00,00,000 (5 Cr)
Rs. 8850* Bonus on Junglee Rummy.
Win Now - http://gmg.im/fZBPBl To Optout SMS JR to 56161"""

sms4 = """Hello Player,
SANDEEP Won Rs.8,16,000 on Howzat!
MI vs LSG is Live.
Play and win from Rs.60 lakhs
Entry Fees - Rs.34
Click - http://gmg.im/fq9U93"""

sms5 = """1 day left! Participate in Coding Ninjas 100% Scholarship Test.
Get up to 100% Scholarship on all courses. Register now weurl.co/4s7rMG
- Coding Ninjas"""

sms6 = """IDBI Bank A/c  credited for INR 92.00 thru UPI.
Bal INR 1136.68 (incl. of chq in clg) as of 25 MAY 19:46hr. If not used by you, call """

sms7 = """Your a/c no. is debited for Rs.100.00 on 20-05-23 and credited to a/c no.
(UPI Ref no ).To block UPI services of IDBI Bank,
Send SMS as UPIBLOCK <type your mobile no> to  from your registered number,
or call immediately.- IDBI BANK"""

sms8 = """IDBI Bank A/c NN13554 credited for INR 10.00 thru UPI. Bal INR 15649.68
(incl. of chq in clg) as of 14 MAY 10:26hr. If not used by you, call """

sms9 = """Enter the word or phrase you want to replace in Find what.
Enter your new text in Replace with. """

sms10 = """Dear Customer, +XXXXXXXX is now available to take calls."""
sms_lis = [sms1,sms2,sms3,sms4,sms5,sms6,sms7,sms8,sms9,sms10]

for i,sms in enumerate(sms_lis):
    res = manager(sms)
    print("sms "+str(i+1),res)